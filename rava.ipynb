{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up Environment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load .env file\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "# Access environment variables\n",
    "speech_key = os.getenv('SPEECH_KEY')\n",
    "print(f'SPEECH_KEY: {speech_key}')\n",
    "speech_endpoint = os.getenv('SPEECH_ENDPOINT')\n",
    "print(f'SPEECH_ENDPOINT: {speech_endpoint}')\n",
    "speech_region = os.getenv('SPEECH_REGION')\n",
    "print(f'REGION: {speech_region}')\n",
    "\n",
    "gpt_key = os.getenv('GPT_KEY')\n",
    "print(f'GPT_KEY: {gpt_key}')\n",
    "gpt_endpoint = os.getenv('GPT_ENDPOINT')\n",
    "print(f'GPT_ENDPOINT: {gpt_endpoint}')\n",
    "gpt_region = os.getenv('OPENAI_REGION')\n",
    "print(f'REGION: {gpt_region}')\n",
    "\n",
    "llama_token = os.getenv('LLAMA_TOKEN')\n",
    "print(f'LLAMA_TOKEN: {llama_token}')\n",
    "\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speech-To-Text Azure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import azure.cognitiveservices.speech as speechsdk\n",
    "\n",
    "speech_config = speechsdk.SpeechConfig(subscription=speech_key, region=speech_region)\n",
    "def recognize_speech():\n",
    "    # This example requires environment variables named \"SPEECH_KEY\" and \"SPEECH_REGION\"\n",
    "    \n",
    "    speech_config.speech_recognition_language=\"fr-FR\"\n",
    "\n",
    "    audio_config = speechsdk.audio.AudioConfig(use_default_microphone=True)\n",
    "    speech_recognizer = speechsdk.SpeechRecognizer(speech_config=speech_config, audio_config=audio_config)\n",
    "\n",
    "   \n",
    "    \n",
    "    print(\"Speak into your microphone.\")\n",
    "    speech_recognition_result = speech_recognizer.recognize_once_async().get()\n",
    "        \n",
    "    if speech_recognition_result.reason == speechsdk.ResultReason.RecognizedSpeech:\n",
    "        print(\"You: {}\".format(speech_recognition_result.text))\n",
    "        return speech_recognition_result.text\n",
    "    elif speech_recognition_result.reason == speechsdk.ResultReason.NoMatch:\n",
    "        print(\"No speech could be recognized: {}\".format(speech_recognition_result.no_match_details))\n",
    "    elif speech_recognition_result.reason == speechsdk.ResultReason.Canceled:\n",
    "        cancellation_details = speech_recognition_result.cancellation_details\n",
    "        print(\"Speech Recognition canceled: {}\".format(cancellation_details.reason))\n",
    "        if cancellation_details.reason == speechsdk.CancellationReason.Error:\n",
    "            print(\"Error details: {}\".format(cancellation_details.error_details))\n",
    "            print(\"Did you set the speech resource key and region values?\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO this code will work, but we need to get the stuff from the microphone/user, not the TTS\n",
    "\n",
    "import azure.cognitiveservices.speech as speechsdk\n",
    "import wave\n",
    "import io\n",
    "\n",
    "class CustomAudioOutputStream(speechsdk.audio.PullAudioOutputStream):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self._buffer = io.BytesIO()\n",
    "\n",
    "    def read(self, size):\n",
    "        # Read the audio data into your buffer\n",
    "        return self._buffer.read(size)\n",
    "\n",
    "    def write(self, data):\n",
    "        # Write data to the buffer to simulate real-time streaming\n",
    "        self._buffer.write(data)\n",
    "\n",
    "    def close(self):\n",
    "        self._buffer.close()\n",
    "\n",
    "# Azure TTS Configuration\n",
    "speech_config = speechsdk.SpeechConfig(subscription=speech_key, region=speech_region)\n",
    "audio_config = speechsdk.audio.AudioOutputConfig(stream=CustomAudioOutputStream())\n",
    "\n",
    "# Create TTS synthesizer\n",
    "synthesizer = speechsdk.SpeechSynthesizer(speech_config=speech_config, audio_config=audio_config)\n",
    "\n",
    "# Synthesize some speech\n",
    "synthesis_result = synthesizer.speak_text_async(\"Hello, this is a test of Azure TTS.\").get()\n",
    "\n",
    "# Use the custom stream to pipe audio data elsewhere\n",
    "output_stream = audio_config.stream\n",
    "output_data = output_stream.read(1024)  # Example read data\n",
    "\n",
    "# Now you can pipe `output_data` to a file, another program, etc.\n",
    "with wave.open(\"output.wav\", \"wb\") as wf:\n",
    "    wf.setnchannels(1)\n",
    "    wf.setsampwidth(2)\n",
    "    wf.setframerate(16000)\n",
    "    wf.writeframes(output_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Make this work so that we can get the properties like the class 'azure.cognitiveservices.speech.SpeechRecognitionResult'\n",
    "\n",
    "import azure.cognitiveservices.speech as speechsdk\n",
    "import wave\n",
    "\n",
    "# Azure Speech SDK configuration\n",
    "\n",
    "speech_config = speechsdk.SpeechConfig(subscription=speech_key, region=speech_region)\n",
    "\n",
    "# Use the default microphone for audio capture\n",
    "audio_config = speechsdk.audio.AudioConfig(use_default_microphone=True)\n",
    "\n",
    "# Create a speech recognizer with microphone as the audio source\n",
    "speech_recognizer = speechsdk.SpeechRecognizer(speech_config=speech_config, audio_config=audio_config)\n",
    "\n",
    "# List to store audio data chunks\n",
    "audio_chunks = []\n",
    "\n",
    "# Callback function for recognizing events\n",
    "def handle_audio_event(evt):\n",
    "    audio_data = evt.result.audio\n",
    "    if audio_data:\n",
    "        audio_chunks.append(audio_data)\n",
    "\n",
    "# Subscribe to recognized audio events\n",
    "speech_recognizer.recognized.connect(handle_audio_event)\n",
    "\n",
    "# Start continuous recognition\n",
    "print(\"Listening to microphone. Press Ctrl+C to stop...\")\n",
    "speech_recognizer.start_continuous_recognition()\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        pass  # Keep listening indefinitely\n",
    "except KeyboardInterrupt:\n",
    "    # Stop recognition on interrupt\n",
    "    speech_recognizer.stop_continuous_recognition()\n",
    "    print(\"Stopping...\")\n",
    "\n",
    "# Combine audio chunks into a single byte stream\n",
    "audio_bytes = b''.join(audio_chunks)\n",
    "\n",
    "# Save the audio to a .wav file\n",
    "output_filename = \"output.wav\"\n",
    "with wave.open(output_filename, 'wb') as wf:\n",
    "    wf.setnchannels(1)  # Mono audio\n",
    "    wf.setsampwidth(2)  # 16-bit audio\n",
    "    wf.setframerate(16000)  # 16 kHz sample rate\n",
    "    wf.writeframes(audio_bytes)\n",
    "\n",
    "print(f\"Audio saved to {output_filename}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MyProsody Speech Rate Detection\n",
    "\n",
    "We need to clone the myprosody repository, I chose to do so locally within the repo, but added it to the gitignore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import myprosody as mysp\n",
    "import io\n",
    "import sys\n",
    "\n",
    "# Redirect the print output\n",
    "def detect_speech_rate(wav: str) -> str:\n",
    "    # Create a StringIO object to capture the output\n",
    "    p=wav.split(\".\")[0]\n",
    "    c=r\"../myprosody/myprosody\"\n",
    "    captured_output = io.StringIO()\n",
    "    sys.stdout = captured_output  # Redirect sys.stdout to the StringIO object\n",
    "    try:\n",
    "        # Call the function whose output you want to capture\n",
    "        mysp.myspsr(p,c)\n",
    "    finally:\n",
    "        sys.stdout = sys.__stdout__  # Restore the original sys.stdout\n",
    "\n",
    "    # Get the captured output as a string\n",
    "    output = captured_output.getvalue()\n",
    "    captured_output.close()  # Close the StringIO object\n",
    "    \n",
    "    # TODO: Figure out what Azure TTS's defaualt speech rate is with myprosody\n",
    "    final_syl_sec = 8\n",
    "    try:\n",
    "        final_syl_sec = output.split(\"rate_of_speech= \")[1].strip()\n",
    "    except Exception as e:\n",
    "        print(output)\n",
    "    \n",
    "    return float(final_syl_sec) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenAI GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(prompt):\n",
    "    \"\"\"Generate a response using Azure OpenAI Service.\"\"\"\n",
    "    response = openai_client.chat_completions.create(\n",
    "        deployment_id=deployment_id,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "    )\n",
    "    return response.choices[0].message[\"content\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLama (test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "\n",
    "# Run the huggingface-cli login command\n",
    "subprocess.run([\"huggingface-cli\", \"login\", \"--token\", llama_token])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!huggingface-cli download meta-llama/Meta-Llama-3-8B-Instruct --include \"original/*\" --local-dir Meta-Llama-3-8B-Instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "\n",
    "model_id = \"meta-llama/Llama-3.1-8B\"\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\", model=model_id, model_kwargs={\"torch_dtype\": torch.bfloat16}, device_map=\"auto\", use_auth_token=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pipeline(\"Hey how are you doing today?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text-to-Speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def speak_response(response):\n",
    "\t\"\"\"Convert text to speech using Azure Speech SDK.\"\"\"\n",
    "\tspeech_config.speech_synthesis_language=\"fr-FR\"\n",
    "\tsynthesizer = speechsdk.SpeechSynthesizer(speech_config=speech_config)\n",
    "    # Define SSML with Speaking Rate\n",
    "\trate = '20%'\n",
    "\t# speech_config.voice_name = \"fr-FR-Julie-Apollo\"\n",
    "\tspeech_config.speech_synthesis_voice_name = \"fr-FR-VivienneMultilingualNeural\"\n",
    "\tssml_string = f\"\"\"<speak version=\"1.0\" xmlns=\"http://www.w3.org/2001/10/synthesis\" \n",
    "    xmlns:mstts=\"http://www.w3.org/2001/mstts\" \n",
    "    \txml:lang=\"en-US\">\n",
    "    \t\t<voice name=\"fr-FR-VivienneMultilingualNeural\">\n",
    "        <prosody rate=\"{rate}\">{response}.</prosody>\n",
    "    </voice>\n",
    "    </speak>\"\"\"\n",
    "\tresult = synthesizer.speak_ssml_async(ssml_string).get()\n",
    "\tif result.reason == speechsdk.ResultReason.SynthesizingAudioCompleted:\n",
    "\t\tprint(\"Speech synthesized successfully.\")\n",
    "\t\treturn result.audio_data\n",
    "\telif result.reason == speechsdk.ResultReason.Canceled:\n",
    "\t\tcancellation_details = result.cancellation_details\n",
    "\t\tprint(f\"Speech synthesis canceled: {cancellation_details.reason}\")\n",
    "\t\tif cancellation_details.reason == speechsdk.CancellationReason.Error:\n",
    "\t\t\tprint(f\"Error details: {cancellation_details.error_details}\")\n",
    "\n",
    "# Synthesize Speech\n",
    "\t# synthesizer.speak_text_async(response)\n",
    "\n",
    "speak_response(\"Moi? Je vais bien, merci!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"Main voice agent loop.\"\"\"\n",
    "    while True:\n",
    "        user_input = recognize_speech()\n",
    "        # if user_input:\n",
    "        #     # Generate a response using Azure OpenAI Service\n",
    "        #     response = generate_response(user_input)\n",
    "        #     print(f\"Agent: {response}\")\n",
    "\n",
    "        #     # Speak the response\n",
    "        #     speak_response(response)\n",
    "        # else:\n",
    "        #     print(\"Could not understand input. Please try again.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import azure.cognitiveservices.speech as speechsdk\n",
    "\n",
    "# def process_recognized_text(text):\n",
    "#     \"\"\"\n",
    "#     This function receives recognized speech as text.\n",
    "#     You can customize it to store, process, or send the output elsewhere.\n",
    "#     \"\"\"\n",
    "#     print(f\"Recognized Text: {text}\")\n",
    "\n",
    "# def recognize_and_pipe_text():\n",
    "#     # Set up Azure Speech config\n",
    "#     speech_key = \"YourAzureSpeechKey\"\n",
    "#     region = \"YourAzureRegion\"\n",
    "#     speech_config = speechsdk.SpeechConfig(subscription=speech_key, region=region)\n",
    "\n",
    "#     # Use microphone as input (can replace with a specific audio file if needed)\n",
    "#     audio_input = speechsdk.audio.AudioConfig(use_default_microphone_input=True)\n",
    "\n",
    "#     # Create the SpeechRecognizer object\n",
    "#     speech_recognizer = speechsdk.SpeechRecognizer(speech_config=speech_config, audio_config=audio_input)\n",
    "\n",
    "#     # Event triggered when a speech recognition result is received\n",
    "#     def recognized_handler(event):\n",
    "#         if event.result.reason == speechsdk.ResultReason.RecognizedSpeech:\n",
    "#             recognized_text = event.result.text\n",
    "#             # Send recognized text to another function\n",
    "#             process_recognized_text(recognized_text)\n",
    "#         elif event.result.reason == speechsdk.ResultReason.NoMatch:\n",
    "#             print(\"No speech could be recognized.\")\n",
    "\n",
    "#     # Connect events to handlers\n",
    "#     speech_recognizer.recognized.connect(recognized_handler)\n",
    "\n",
    "#     # Start continuous recognition\n",
    "#     print(\"Start speaking...\")\n",
    "#     speech_recognizer.start_continuous_recognition()\n",
    "\n",
    "#     # Keep recognizing (use stop_continuous_recognition() to exit cleanly)\n",
    "#     try:\n",
    "#         while True:\n",
    "#             pass  # Keeps the application running to listen for speech input\n",
    "#     except KeyboardInterrupt:\n",
    "#         print(\"Stopping recognition.\")\n",
    "#         speech_recognizer.stop_continuous_recognition()\n",
    "\n",
    "# recognize_and_pipe_text()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rava",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
