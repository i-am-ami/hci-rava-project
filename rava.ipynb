{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up Environment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPEECH_KEY: 17HtlObtvPDDrkEospAE53eY6KKmihIg6u7mWttAlpsznvqOehAYJQQJ99ALAC1i4TkXJ3w3AAAYACOGiLzZ\n",
      "SPEECH_ENDPOINT: https://centralus.api.cognitive.microsoft.com/\n",
      "REGION: centralus\n",
      "GPT_KEY: 717TGQ1puzD6S80QLf4tGJ25OT8eWZLmsRsAMsHMpEd5Rf1Kkxg7JQQJ99ALACLArgHXJ3w3AAABACOG6jSk\n",
      "GPT_ENDPOINT: https://openai-gpt-lm.openai.azure.com/\n",
      "REGION: southcentralus\n",
      "LLAMA_TOKEN: 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\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load .env file\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "# Access environment variables\n",
    "speech_key = os.getenv('SPEECH_KEY')\n",
    "print(f'SPEECH_KEY: {speech_key}')\n",
    "speech_endpoint = os.getenv('SPEECH_ENDPOINT')\n",
    "print(f'SPEECH_ENDPOINT: {speech_endpoint}')\n",
    "speech_region = os.getenv('SPEECH_REGION')\n",
    "print(f'REGION: {speech_region}')\n",
    "\n",
    "gpt_key = os.getenv('GPT_KEY')\n",
    "print(f'GPT_KEY: {gpt_key}')\n",
    "gpt_endpoint = os.getenv('GPT_ENDPOINT')\n",
    "print(f'GPT_ENDPOINT: {gpt_endpoint}')\n",
    "gpt_region = os.getenv('OPENAI_REGION')\n",
    "print(f'REGION: {gpt_region}')\n",
    "\n",
    "llama_token = os.getenv('LLAMA_TOKEN')\n",
    "print(f'LLAMA_TOKEN: {llama_token}')\n",
    "\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speech-To-Text Azure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azure.cognitiveservices.speech as speechsdk\n",
    "speech_config = speechsdk.SpeechConfig(subscription=speech_key, region=speech_region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wave\n",
    "def write_to_wav(filename, frame_rate, audio_data):\n",
    "    with wave.open(filename, \"wb\") as wf:\n",
    "        wf.setnchannels(1)  # Mono audio\n",
    "        wf.setsampwidth(2)  # Sample width in bytes (16-bit audio)\n",
    "        wf.setframerate(frame_rate)\n",
    "        wf.writeframes(audio_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dual Audio Detection Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sounddevice as sd\n",
    "import wave\n",
    "import azure.cognitiveservices.speech as speechsdk\n",
    "\n",
    "class DualAudioStream:\n",
    "    def __init__(self, rate=16000, chunk=1024):\n",
    "        self.rate = rate\n",
    "        self.chunk = chunk\n",
    "        self.frames = []\n",
    "        self.push_audio_input_stream = speechsdk.audio.PushAudioInputStream()\n",
    "\n",
    "    def callback(self, indata, frames, time, status):\n",
    "        \"\"\"Sounddevice audio callback to capture and push audio data.\"\"\"\n",
    "        if status:\n",
    "            print(f\"Sounddevice input status: {status}\")\n",
    "        # Save audio frames for WAV file\n",
    "        self.frames.append(indata.copy())\n",
    "        # Write audio to PushAudioInputStream (required by Azure SDK)\n",
    "        self.push_audio_input_stream.write(indata.tobytes())\n",
    "\n",
    "    def start_recording(self):\n",
    "        \"\"\"Start streaming audio from the microphone.\"\"\"\n",
    "        print(\"Recording audio...\")\n",
    "        self.stream = sd.InputStream(\n",
    "            samplerate=self.rate,\n",
    "            channels=1,\n",
    "            dtype='int16',\n",
    "            blocksize=self.chunk,\n",
    "            callback=self.callback,\n",
    "        )\n",
    "        self.stream.start()\n",
    "\n",
    "    def stop_recording(self):\n",
    "        \"\"\"Stop streaming audio and close resources.\"\"\"\n",
    "        self.stream.stop()\n",
    "        self.stream.close()\n",
    "        print(\"Audio streaming stopped.\")\n",
    "\n",
    "    def save_to_wav(self, file_name):\n",
    "        \"\"\"Save recorded audio to a WAV file.\"\"\"\n",
    "        audio_data = b\"\".join(frame.tobytes() for frame in self.frames)\n",
    "        with wave.open(file_name, \"wb\") as wf:\n",
    "            wf.setnchannels(1)  # Mono audio\n",
    "            wf.setsampwidth(2)  # 16-bit audio\n",
    "            wf.setframerate(self.rate)\n",
    "            wf.writeframes(audio_data)\n",
    "        print(f\"Audio saved to {file_name}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Speech Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recognize_speech():\n",
    "    # This example requires environment variables named \"SPEECH_KEY\" and \"SPEECH_REGION\"\n",
    "    \n",
    "    speech_config.speech_recognition_language=\"fr-FR\"\n",
    "\n",
    "    audio_config = speechsdk.audio.AudioConfig(use_default_microphone=True)\n",
    "    speech_recognizer = speechsdk.SpeechRecognizer(speech_config=speech_config, audio_config=audio_config)\n",
    "\n",
    "    # Create an instance of the DualAudioStream\n",
    "    dual_audio = DualAudioStream()\n",
    "\n",
    "    try:\n",
    "        # Start recording audio from the microphone\n",
    "        dual_audio.start_recording()\n",
    "\n",
    "        # Configure Azure SpeechRecognizer with PushAudioInputStream\n",
    "        audio_config = speechsdk.audio.AudioConfig(stream=dual_audio.push_audio_input_stream)\n",
    "        recognizer = speechsdk.SpeechRecognizer(speech_config=speech_config, audio_config=audio_config)\n",
    "\n",
    "        # Start recognition\n",
    "        print(\"Speak into your microphone...\")\n",
    "        speech_recognition_result = recognizer.recognize_once_async().get()\n",
    "\n",
    "        # Handle recognition result\n",
    "        if speech_recognition_result.reason == speechsdk.ResultReason.RecognizedSpeech:\n",
    "            print(\"You: {}\".format(speech_recognition_result.text))\n",
    "            print(\"Recognized: {}\".format(speech_recognition_result.duration))\n",
    "            return speech_recognition_result.text\n",
    "        elif speech_recognition_result.reason == speechsdk.ResultReason.NoMatch:\n",
    "            print(\"No speech could be recognized: {}\".format(speech_recognition_result.no_match_details))\n",
    "        elif speech_recognition_result.reason == speechsdk.ResultReason.Canceled:\n",
    "            cancellation_details = speech_recognition_result.cancellation_details\n",
    "            print(\"Speech Recognition canceled: {}\".format(cancellation_details.reason))\n",
    "            if cancellation_details.reason == speechsdk.CancellationReason.Error:\n",
    "                print(\"Error details: {}\".format(cancellation_details.error_details))\n",
    "                print(\"Did you set the speech resource key and region values?\")\n",
    "    finally:\n",
    "        dual_audio.stop_recording()\n",
    "        dual_audio.save_to_wav(\"output.wav\")\n",
    "        print(\"Finished.\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MyProsody Speech Rate Detection\n",
    "\n",
    "We need to clone the myprosody repository, I chose to do so locally within the repo, but added it to the gitignore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import myprosody as mysp\n",
    "import io\n",
    "import sys\n",
    "\n",
    "def detect_sr(src: str) -> int:\n",
    "    # Create a StringIO object to capture the output\n",
    "    p=src\n",
    "    c=r\"../myprosody/myprosody\"\n",
    "\n",
    "    captured_output = io.StringIO()\n",
    "    sys.stdout = captured_output  # Redirect sys.stdout to the StringIO object\n",
    "    try:\n",
    "        # Call the function whose output you want to capture\n",
    "        mysp.myspsr(p,c)\n",
    "    finally:\n",
    "        sys.stdout = sys.__stdout__  # Restore the original sys.stdout\n",
    "\n",
    "    # Get the captured output as a string\n",
    "    output = captured_output.getvalue()\n",
    "    captured_output.close()  # Close the StringIO object\n",
    "    \n",
    "    final_syl_sec = 8\n",
    "    try:\n",
    "        final_syl_sec = int(output.split(\" \")[1].strip())\n",
    "    except Exception as e:\n",
    "        print(output)\n",
    "    \n",
    "    return final_syl_sec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenAI GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(prompt):\n",
    "    \"\"\"Generate a response using Azure OpenAI Service.\"\"\"\n",
    "    response = openai_client.chat_completions.create(\n",
    "        deployment_id=deployment_id,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "    )\n",
    "    return response.choices[0].message[\"content\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLama (test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "\n",
    "# Run the huggingface-cli login command\n",
    "subprocess.run([\"huggingface-cli\", \"login\", \"--token\", llama_token])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!huggingface-cli download meta-llama/Meta-Llama-3-8B-Instruct --include \"original/*\" --local-dir Meta-Llama-3-8B-Instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "\n",
    "model_id = \"meta-llama/Llama-3.1-8B\"\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\", model=model_id, model_kwargs={\"torch_dtype\": torch.bfloat16}, device_map=\"auto\", use_auth_token=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pipeline(\"Hey how are you doing today?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text-to-Speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def speak_response(response):\n",
    "\t\"\"\"Convert text to speech using Azure Speech SDK.\"\"\"\n",
    "\tspeech_config.speech_synthesis_language=\"fr-FR\"\n",
    "\tsynthesizer = speechsdk.SpeechSynthesizer(speech_config=speech_config)\n",
    "    # Define SSML with Speaking Rate\n",
    "\trate = '20%'\n",
    "\t# speech_config.voice_name = \"fr-FR-Julie-Apollo\"\n",
    "\tspeech_config.speech_synthesis_voice_name = \"fr-FR-VivienneMultilingualNeural\"\n",
    "\tssml_string = f\"\"\"<speak version=\"1.0\" xmlns=\"http://www.w3.org/2001/10/synthesis\" \n",
    "    xmlns:mstts=\"http://www.w3.org/2001/mstts\" \n",
    "    \txml:lang=\"en-US\">\n",
    "    \t\t<voice name=\"fr-FR-VivienneMultilingualNeural\">\n",
    "        <prosody rate=\"{rate}\">{response}.</prosody>\n",
    "    </voice>\n",
    "    </speak>\"\"\"\n",
    "\tresult = synthesizer.speak_ssml_async(ssml_string).get()\n",
    "\tif result.reason == speechsdk.ResultReason.SynthesizingAudioCompleted:\n",
    "\t\tprint(\"Speech synthesized successfully.\")\n",
    "\t\treturn result.audio_data\n",
    "\telif result.reason == speechsdk.ResultReason.Canceled:\n",
    "\t\tcancellation_details = result.cancellation_details\n",
    "\t\tprint(f\"Speech synthesis canceled: {cancellation_details.reason}\")\n",
    "\t\tif cancellation_details.reason == speechsdk.CancellationReason.Error:\n",
    "\t\t\tprint(f\"Error details: {cancellation_details.error_details}\")\n",
    "\n",
    "# Synthesize Speech\n",
    "\t# synthesizer.speak_text_async(response)\n",
    "\n",
    "speak_response(\"Moi? Je vais bien, merci!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording audio...\n",
      "Speak into your microphone...\n",
      "You: This is going to records home audio will see where it goes.\n",
      "Recognized: 23200000\n",
      "Audio streaming stopped.\n",
      "Audio saved to output.wav.\n",
      "Finished.\n",
      "Info: on_underlying_io_bytes_received: Close frame received\n",
      "Info: on_underlying_io_bytes_received: closing underlying io.\n",
      "Info: on_underlying_io_close_complete: uws_state: 6.\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \"\"\"Main voice agent loop.\"\"\"\n",
    "    user_input = recognize_speech()\n",
    "        # if user_input:\n",
    "        #     # Generate a response using Azure OpenAI Service\n",
    "        #     response = generate_response(user_input)\n",
    "        #     print(f\"Agent: {response}\")\n",
    "\n",
    "        #     # Speak the response\n",
    "        #     speak_response(response)\n",
    "        # else:\n",
    "        #     print(\"Could not understand input. Please try again.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hci-rava-app",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
